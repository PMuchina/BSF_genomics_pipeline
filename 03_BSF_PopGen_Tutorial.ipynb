{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ff3d92b",
   "metadata": {},
   "source": [
    "# ðŸª° Genetic Diversity and Population Structure of BSF  ðŸª° \n",
    "\n",
    "This tutorial guides you through the complete workflow for analyzing genetic diversity and population structure in **Black Soldier Fly (BSF)** populations using whole-genome sequencing data.\n",
    "\n",
    "ðŸ”” **Note:** **Step 6 (Phasing and Imputation is optional)**. Include it when you have low-coverage whole-genome sequence data. In this case, I used the high-coverage SNPs (VCF) as a reference panel to impute the low-coverage data (sequence at 1x)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea699e54",
   "metadata": {},
   "source": [
    "## ðŸ§° Setup Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b45da4-4ba7-4d38-b582-3092e61aa8ad",
   "metadata": {},
   "source": [
    "!conda create -n bsf_genomics fastqc bwa samtools bcftools gatk picard vcftools plink beagle r-quilt admixture treemix -c bioconda -c conda-forge \\\n",
    "!conda activate bsf_genomics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb4a84e",
   "metadata": {},
   "source": [
    "## ðŸ“ Input Files\n",
    "- Paired-end FASTQ files: [Project: PRJEB58720](https://www.ebi.ac.uk/ena/browser/view/PRJEB58720), [DOI](https://doi.org/10.1101/2023.10.21.563413)\n",
    "- Reference genome: [BSF_reference.fasta](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_905115235.1/)\n",
    "- Sample metadata: `sample_pop.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18f73a6",
   "metadata": {},
   "source": [
    "## Quality Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0853ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd 2_fastqc\n",
    "\n",
    "# Create soft links for all the files in 1_data\n",
    "for fastq in ../Data/*.fastq.gz; do\n",
    "  ln -s $fastq\n",
    "done\n",
    "\n",
    "# Run fastqc on the files\n",
    "fastqc -t 4 *.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae68bbf",
   "metadata": {},
   "source": [
    "## Mapping to Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087a58d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#Index reference genome\n",
    "bwa index reference_genome.fasta\n",
    "\n",
    "# Path to the reference genome indexed with BWA\n",
    "reference_genome=\"path/to/ref\"\n",
    "\n",
    "# Loop through each forward-read file\n",
    "# Change the file extension if its not R1.fastq.gz || R2.fastq.gz \n",
    "\n",
    "for forward_file in ../Data/*.R1.fastq.gz; do\n",
    "     # Get the base filename without the extension\n",
    "     base_filename=$(basename -- \"$forward_file\" | sed 's/.R1.fq.gz//')\n",
    "\n",
    "     # Generate the paths for the forward and reverse-read files\n",
    "     reverse_file=\"../1_data/${base_filename}.R2.fq.gz\"\n",
    "\n",
    "     echo \"Processing: ${base_filename}\"\n",
    "\n",
    "     # Perform the alignment using bwa mem, convert the sam to bam, and sort the bam\n",
    "     # Change the read group according to read description and the sequencing platform used.\n",
    "     bwa mem -M -t 8 -R \"@RG\\tID:${base_filename}\\tPL:BGISEQ-500\\tSM:${base_filename}\" \\\n",
    "     \"$reference_genome\" \"$forward_file\" \"$reverse_file\" | \\\n",
    "     samtools view -bS | \\\n",
    "     samtools sort --output-fmt BAM -@ 8 -o \"${base_filename}.sorted.bam\" -\n",
    "\n",
    "     # Index the sorted BAM file\n",
    "     samtools index ${base_filename}.sorted.bam\n",
    "\n",
    "\n",
    "    echo \"Done Processing: ${base_filename}\"\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed195ba",
   "metadata": {},
   "source": [
    "## Mark Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad791af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Input directory containing BAM files\n",
    "bam_directory=\"path/to/bamfiles\"\n",
    "\n",
    "# Output directory for processed BAM files\n",
    "output_directory=\"./\"\n",
    "\n",
    "# Check if the input directory exists\n",
    "if [ ! -d \"$bam_directory\" ]; then\n",
    "    echo \"Input directory $bam_directory does not exist\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "mkdir -p \"$output_directory\"\n",
    "\n",
    "# Iterate over each .bam file in the input directory\n",
    "for bam in \"$bam_directory\"*.bam; do\n",
    "    if [ -f \"$bam\" ]; then\n",
    "        base_name=$(basename \"${bam%.*}\")\n",
    "        echo \"Processing ${bam}\"\n",
    "\n",
    "        java -Xmx10G -jar picard.jar MarkDuplicates \\\n",
    "                    I=\"${bam}\" \\\n",
    "                    O=\"${output_directory}${base_name}_markdup.bam\" \\\n",
    "                    M=\"${output_directory}${base_name}_markdup_metrics.txt\" \\\n",
    "                    OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 \\\n",
    "                    CREATE_INDEX=true \\\n",
    "                    VALIDATION_STRINGENCY=LENIENT \\\n",
    "                    REMOVE_DUPLICATES=true \\\n",
    "                    ASSUME_SORT_ORDER=coordinate\n",
    "\n",
    "      else\n",
    "        echo \"No .bam files found in $bam_directory\"\n",
    "        exit 1\n",
    "    fi\n",
    "\n",
    "done\n",
    "\n",
    "echo \"Completed marking duplicates\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa19a41b",
   "metadata": {},
   "source": [
    "## Variant Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e406aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#Running freebayes in parrallel\n",
    "reference_genome=\"path/to/ref\"\n",
    "\n",
    "# Create a list with all your bam files\n",
    "ls -l \"path/to/markdup.bam\" | awk '{print $NF}' | xargs -I{} readlink -f {} > bamlist\n",
    "\n",
    "# Run freebayes\n",
    "# Here the genome is chunked into regions of 100000 (1mb). \n",
    "# Ran on 20 processor (20 jobs will be ran in parallel).\n",
    "# This can be adjusted based on the size of the genome and the computational resources\n",
    "\n",
    "freebayes-parallel \\\n",
    "   <(fasta_generate_regions.py ${ref}.fai 100000) 20 \\\n",
    "   --fasta-reference ${reference_genome}  \\\n",
    "   --bam-list bamlist  > output.vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf45965f",
   "metadata": {},
   "source": [
    "## VCF Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed302103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#SNP filtering tutorial: https://ddocent.com/filtering/\n",
    "\n",
    "bcftools index --threads 8 output.vcf.gz\n",
    "\n",
    "#step 1: filters on high missignesss (>50%), minor allele count (mac3), and Quality (Q30)\n",
    "vcftools --gzvcf output.vcf.gz  \\\n",
    "         --max-missing 0.5 \\\n",
    "         --mac 3 \\\n",
    "         --minQ 30 \\\n",
    "         --recode \\\n",
    "         --recode-INFO-all \\\n",
    "         --stdout | bgzip -c --threads 8 > output.mac3.Q30.vcf.gz\n",
    "\n",
    "#step 2: Remove individuals with high missing data (> 50%)\n",
    "vcftools --gzvcf output.mac3.Q30.vcf.gz --missing-indv\n",
    "mawk '!/IN/' out.imiss | cut -f5 > totalmissing\n",
    "mawk '$5 > 0.5' out.imiss | cut -f1 > lowDP.indv\n",
    "\n",
    "vcftools --gzvcf output.mac3.Q30.vcf.gz \\\n",
    "         --remove lowDP.indv \\\n",
    "         --recode \\\n",
    "         --recode-INFO-all \\\n",
    "         --stdout | bgzip -c --threads 8 > output.mac3.Q30.lDP.vcf.gz\n",
    "\n",
    "#Step 3: Decompose biallelic block substitutions\n",
    "# More info on vt here: https://genome.sph.umich.edu/wiki/Vt#Decompose_biallelic_block_substitutions\n",
    "bcftools index --threads 8 output.mac3.Q30.lDP.vcf.gz\n",
    "\n",
    "vt decompose_blocksub output.mac3.Q30.lDP.vcf.gz | \\\n",
    "vt uniq - | \\\n",
    "bgzip -c --threads 8 > output.mac3.Q30.lDP.dec.dup.vcf.gz\n",
    "\n",
    "bcftools index --threads 8 output.mac3.Q30.lDP.dec.dup.vcf.gz\n",
    "\n",
    "#Step 4: Remove Indels and multiallelic (--min-alleles/--max-alleles 2),Kept only biallelic SNPs\n",
    "bcftools view -m2 -M2 -v output.mac3.Q30.lDP.dec.dup.vcf.gz \\\n",
    "                     -Oz -o output.mac3.Q30.lDP.dec.dup.snps.vcf.gz --threads 8\n",
    "\n",
    "bcftools index --threads 8 output.mac3.Q30.lDP.dec.dup.snps.vcf.gz\n",
    "\n",
    "#Step 5: Filter on meanDP,adjust based on your data.\n",
    "vcftools --gzvcf output.mac3.Q30.lDP.dec.dup.snps.vcf.gz \\\n",
    "         --min-meanDP 5 \\\n",
    "         --max-meanDP 25 \\\n",
    "         --recode \\\n",
    "         --recode-INFO-all \\\n",
    "         --stdout | bgzip -c --threads 8 > output.mac3.Q30.lDP.dec.dup.snps.mnDP.vcf.gz\n",
    "\n",
    "bcftools index --threads 8 output.mac3.Q30.lDP.dec.dup.snps.mnDP.vcf.gz\n",
    "\n",
    "#Step 6: Missigness, adjust based on your data and downstream analysis intended\n",
    "vcftools --gzvcf output.mac3.Q30.lDP.dec.dup.snps.mnDP.vcf.gz \\\n",
    "         --max-missing 0.95 \\\n",
    "         --recode \\\n",
    "         --recode-INFO-all \\\n",
    "         --stdout | bgzip -c --threads 8 > output.mac3.Q30.lDP.dec.dup.snps.mnDP.miss0.95.vcf.gz\n",
    "\n",
    "bcftools index --threads 8 output.mac3.Q30.lDP.dec.dup.snps.mnDP.miss0.95.vcf.gz\n",
    "\n",
    "#Step 7: Pop-specific missigness (Based on the number of populations you have) (optional)\n",
    "# Downdload the script here: https://www.ddocent.com/filtering/\n",
    "\n",
    "bash pop_missing_filter.sh output.mac3.Q30.lDP.dec.dup.snps.mnDP.miss0.95.vcf.gz popmap 0.1 3 output.mac3.Q30.lDP.dec.dup.snps.mnDP.miss0.95.pop\n",
    "\n",
    "bgzip --threads 8 output.mac3.Q30.lDP.dec.dup.snps.mnDP.miss0.95.pop.recode.vcf\n",
    "\n",
    "bcftools index --threads 8 output.mac3.Q30.lDP.dec.dup.snps.mnDP.miss0.95.pop.recode.vcf.gz\n",
    "\n",
    "#Step 8: Allelic balance, adjust based on your data\n",
    "bcftools filter -i '(INFO/AB > 0.25 && INFO/AB < 0.75) || INFO/AB < 0.01' -Oz -o output.mac3.Q30.lDP.dec.dup.snps.mnDP.miss0.95.pop.AB.vcf.gz \\\n",
    "                                                                                 output.mac3.Q30.lDP.dec.dup.snps.mnDP.miss0.95.pop.recode.vcf.gz --threads 8\n",
    "\n",
    "bcftools index --threads 8 output.mac3.Q30.lDP.dec.dup.snps.mnDP.miss0.95.pop.AB.vcf.gz\n",
    "\n",
    "#Step 9: Minor Allele Frequency (MAF) threshold to exclude monomorphic sites (which have a MAF of 0).\n",
    "vcftools --gzvcf output.mac3.Q30.lDP.dec.dup.snps.mnDP.miss0.95.pop.AB.vcf.gz \\\n",
    "         --maf 0.001 \\\n",
    "         --recode \\\n",
    "         --recode-INFO-all \\\n",
    "         --stdout | bgzip -c --threads 8 > output.mac3.Q30.lDP.dec.dup.snps.mnDP.miss0.95.pop.AB.lmaf.vcf.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aaa2fa",
   "metadata": {},
   "source": [
    "## Phasing (_optional_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfad62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#Phasing one chromosome at a time\n",
    "VCF='Chr1.mac3.Q30.lDP.dec.dup.snps.mnDP.miss0.95.pop.AB.lmaf.vcf.gz'  # Change this to your input VCF file path\n",
    "Chr='Chr1'         # Change this to your desired chromosome prefix (e.g., Chr1)\n",
    "\n",
    "#Step 1: Phase the VCF using Beagle\n",
    "java -Xmx100g -jar beagle/beagle5.jar gt=${VCF} \\\n",
    "                   out=${Chr}_phased gp=true burnin=10 iterations=40 impute=false nthreads=40 chrom=${Chr}\n",
    "\n",
    "#Step 2: Index the phased VCF output\n",
    "bcftools index --threads 8 ${Chr}_phased.vcf.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f5ed26-71ec-4cae-9c50-f129c58ff923",
   "metadata": {},
   "source": [
    "## Imputation (_optional_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def9b28b-c66a-42f2-8917-7c5d08725008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#Imputation using QUILT v1.0.5: https://github.com/rwdavies/QUILT/blob/master/README_QUILT2.org\n",
    "#Step 1: Building a reference panel\n",
    "\n",
    "Chr=\"Chr1\"\n",
    "\n",
    "# Define paths\n",
    "VCF_file=\"${Chr}_reference_panel.bcf\"\n",
    "chunks_file=\"chunks.${Chr}.txt\" # dat <- QUILT::quilt_chunk_map(\"Chr1\", \"genetic_map.txt\")\n",
    "Out_dir=\"/Quilt_${Chr}/\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "mkdir -p \"$Out_dir\"\n",
    "\n",
    "# Get the chunk from the array task ID\n",
    "chunk_info=$(tail -n +2 \"$chunks_file\" | sed -n \"$((SLURM_ARRAY_TASK_ID + 1))p\")\n",
    "chunk=$(echo \"$chunk_info\" | cut -f1)\n",
    "chr=$(echo \"$chunk_info\" | cut -f2)\n",
    "region=$(echo \"$chunk_info\" | cut -f3)\n",
    "\n",
    "# Extract region start and end\n",
    "regionStart=$(echo \"$region\" | cut -d\":\" -f2 | cut -d\"-\" -f1)\n",
    "regionEnd=$(echo \"$region\" | cut -d\"-\" -f2)\n",
    "\n",
    "# Check if output exists, skip if found\n",
    "if [ ! -f \"$Out_dir/RData/QUILT_prepared_reference.${Chr}.$regionStart.$regionEnd.RData\" ]; then\n",
    "  echo \"Processing chunk $chunk for $Chr (region $regionStart-$regionEnd)\"\n",
    "\n",
    "  # Run QUILT2_prepare_reference.R\n",
    "  QUILT2_prepare_reference.R \\\n",
    "  --outputdir=\"$Out_dir\" \\\n",
    "  --chr=\"$Chr\" \\\n",
    "  --nGen=100 \\\n",
    "  --regionStart=\"$regionStart\" \\\n",
    "  --regionEnd=\"$regionEnd\" \\\n",
    "  --buffer=500000 \\\n",
    "  --reference_vcf_file=\"$VCF_file\"\n",
    "\n",
    "else\n",
    "  echo \"Skipping chunk $chunk for $Chr as output already exists.\"\n",
    "fi\n",
    "\n",
    "################################################################################\n",
    "#Step 2: Imputation\n",
    "# Set the chromosome from the input argument\n",
    "Chr=$1\n",
    "\n",
    "set -e   # Exit immediately if a command exits with a non-zero status\n",
    "\n",
    "echo \"Job started at $(date '+%d_%m_%y_%H_%M_%S')\"\n",
    "\n",
    "# Define coverages and paths\n",
    "coverages=(\"1x\" )\n",
    "num_coverages=${#coverages[@]}\n",
    "chunks_file=\"/${Chr}/chunks.${Chr}.txt\"\n",
    "\n",
    "# Calculate chunk and coverage index based on the array task ID\n",
    "chunk_index=$((SLURM_ARRAY_TASK_ID / num_coverages))\n",
    "coverage_index=$((SLURM_ARRAY_TASK_ID % num_coverages))\n",
    "\n",
    "# Read chunk and region based on chunk_index\n",
    "chunk=$(awk \"NR==$((chunk_index + 2))\" \"$chunks_file\" | cut -f1)  # +2 to skip header\n",
    "region=$(awk \"NR==$((chunk_index + 2))\" \"$chunks_file\" | cut -f3)  # Use 3rd column for region\n",
    "Coverage=${coverages[$coverage_index]}\n",
    "\n",
    "# Extract start and end positions from region\n",
    "if [[ \"$region\" =~ ^[^:]+:([0-9]+)-([0-9]+)$ ]]; then\n",
    "    regionStart=\"${BASH_REMATCH[1]}\"\n",
    "    regionEnd=\"${BASH_REMATCH[2]}\"\n",
    "else\n",
    "    echo \"Error: Invalid region format in $chunks_file on line $((chunk_index + 2))\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Debug output to verify values\n",
    "echo \"Chunk: $chunk, Region: $region, Start: $regionStart, End: $regionEnd, Coverage: $Coverage\"\n",
    "\n",
    "# Define paths (build reference panel, bamlist of samples to be imputed, and output directory)\n",
    "Ref=\"/Quilt_${Chr}/RData/\"\n",
    "Bamlist=\"bamlist\"\n",
    "Out_dir=\"/${Chr}/Quilt/\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "mkdir -p \"$Out_dir\"\n",
    "\n",
    "# Check if the output file already exists to avoid reprocessing\n",
    "if [ ! -f \"$Out_dir/quilt.${Chr}.$regionStart.$regionEnd.vcf.gz\" ]; then\n",
    "    echo \"Processing $Chr, Coverage $Coverage, Chunk $chunk, Region ${regionStart}-${regionEnd}\"\n",
    "\n",
    "    # Run Quilt2 Imputation\n",
    "    QUILT.R \\\n",
    "      --prepared_reference_filename=\"${Ref}QUILT_prepared_reference.${Chr}.${regionStart}.${regionEnd}.RData\" \\\n",
    "      --bamlist=\"${Bamlist}\" \\\n",
    "      --method=diploid \\\n",
    "      --chr=\"$Chr\" \\\n",
    "      --regionStart=\"${regionStart}\" \\\n",
    "      --regionEnd=\"${regionEnd}\" \\\n",
    "      --buffer=500000 \\\n",
    "      --output_filename=\"${Out_dir}/quilt.${Chr}.${regionStart}.${regionEnd}.vcf.gz\" \\\n",
    "      --nCores=1\n",
    "else\n",
    "    echo \"Skipping $Chr, Coverage $Coverage, Chunk $chunk: Output already exists.\"\n",
    "fi\n",
    "\n",
    "echo \"Job completed for ${Chr} at $(date '+%d_%m_%y_%H_%M_%S')\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30a531e",
   "metadata": {},
   "source": [
    "## Population Structure Analysis\n",
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aeb416",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                Genotypic data quality control and PCA analysis\n",
    "################################################################################\n",
    "\n",
    "# Use PLINK to pre-process (i.e. quality control) of BSF data. \n",
    "# Perform PCA analysis, and then make PCA plot using ggplot2 \n",
    "\n",
    "####################\n",
    "# Lets get started \n",
    "###################\n",
    "\n",
    "# We will use genotypic data in VCF format.\n",
    "# Plink options\n",
    "setwd(\"/PCA/\")\n",
    "#system(\"plink-1.9-rc --help\") # to get more info on plink options\n",
    "\n",
    "################################################################################\n",
    "#  For detailed information about options visit link below\n",
    "#                           https://www.cog-genomics.org/plink/1.9/data\n",
    "################################################################################\n",
    "\n",
    "# lets start with VCF and recode into plink format for down stream analysis\n",
    "\n",
    "system(\"plink-1.9-rc --vcf output.mac3.Q30.lDP.dec.dup.snps.mnDP.miss0.95.pop.AB.lmaf.vcf.gz --recode --keep-allele-order --nonfounders --allow-no-sex --double-id --out output\")\n",
    "\n",
    "#list.files(\"./\")\n",
    "\n",
    "# lets make .bed formated plink file\n",
    "system(\"plink-1.9-rc --file output --make-bed --nonfounders --allow-no-sex --keep-allele-order --double-id --out output_bed\")\n",
    "\n",
    "# QC options \n",
    "\n",
    "# 1. Missingness per SNP: --geno  \n",
    "# 2. Missingness per individual: --mind \n",
    "# 3. Minor allele frequency: --maf\n",
    "\n",
    "#20% mSNP, 20% mind, maf 5%\n",
    "system(\"plink-1.9-rc --bfile output_bed --geno 0.2 --mind 0.2 --maf 0.05 --make-bed --keep-allele-order --double-id --out output_qc_out\")\n",
    "\n",
    "# lets make vcf file\n",
    "system(\"plink-1.9-rc --bfile output_qc_out --recode vcf-iid --double-id --keep-allele-order --out output_qc_out_vcf\")\n",
    "\n",
    "# Now, we have a fully filtered VCF we can start some analyses with it.\n",
    "# First of all we will investigate population genetic structure using principal component analysis (PCA).\n",
    "#\n",
    "# PCA analysis: a model-free method -------------------------------------------------------------\n",
    "\n",
    "# one of the assumptions of PCA analysis is that SNP data we use is independent (i.e., there are no spurious correlation among measured variables)\n",
    "# this is not true for most of SNP dataset as allele frequencies are correlated due to physical linkage and linkage disequilibrium .\n",
    "\n",
    "# step 1 identify prune sites\n",
    "# window of 50 Kb\n",
    "# window step size 10\n",
    "# r2 threshold 0.2\n",
    "\n",
    "system(\n",
    "  \"plink-1.9-rc --vcf output_qc_out_vcf.vcf --double-id --allow-extra-chr --set-missing-var-ids @:# --recode vcf-iid --indep-pairwise 50 10 0.2 --out output_PRUNED\"\n",
    ")\n",
    "\n",
    "# step 2: PCA analysis\n",
    "system(\n",
    "  \"plink-1.9-rc --vcf output_qc_out_vcf.vcf --double-id --allow-extra-chr --set-missing-var-ids @:# --extract output_PRUNED.prune.in --pca --make-bed --out output_prune_pca\"\n",
    ")\n",
    "\n",
    "# pca plot\n",
    "library(tidyverse)\n",
    "\n",
    "plinkPCA <- read_table(\"output_prune_pca.eigenvec\", col_names = F)\n",
    "plinkPCA <- plinkPCA[,c(-1,-2)] # remove first two columns\n",
    "EigenValue <- scan(\"output_prune_pca.eigenval\")\n",
    "#view(EigenValue)\n",
    "\n",
    "# set columns names\n",
    "names(plinkPCA)[1:ncol(plinkPCA)] <- paste0(\"PC\", 1:(ncol(plinkPCA)))\n",
    "\n",
    "\n",
    "# percentage variance explained\n",
    "pve <- data.frame(PC = 1:20, pve = EigenValue/sum(EigenValue)*100)\n",
    "#view(pve)\n",
    "\n",
    "# PCA plot\n",
    "\n",
    "# make plot\n",
    "P <- ggplot(pve, aes(PC, pve)) + geom_bar(stat = \"identity\")\n",
    "p1 <- P + ylab(\"Percentage variance explained\") + theme_light()\n",
    "ggsave(\"Percentage.variance.png\", plot = p1, width = 8, height = 6, dpi = 300)\n",
    "\n",
    "\n",
    "# plot pca\n",
    "p2 <- ggplot(plinkPCA, aes(PC1, PC2)) + geom_point(size = 3)+ coord_equal() +\n",
    "   theme_light()+\n",
    "   coord_equal() +\n",
    "   theme_light() + xlab(paste0(\"PC1 (\", signif(pve$pve[1], 3), \"%)\")) + ylab(paste0(\"PC2 (\", signif(pve$pve[2], 3), \"%)\"))\n",
    "\n",
    "ggsave(\"PCA1.png\", plot = p2, width = 8, height = 6, dpi = 300)\n",
    "\n",
    "# lets divide population in two group\n",
    "# (pop <- rep(c(\"A\",\"B\"), each=18))\n",
    "#  pop <- as.data.frame(pop)\n",
    "#  plinkPCA$pop <- pop$pop\n",
    " mypop = read_tsv(\"wild_captives.txt\", col_names = F)\n",
    " \n",
    " plinkPCA$status <- mypop$X3\n",
    " plinkPCA$pop <- mypop$X5\n",
    " \n",
    "p3 <- ggplot(plinkPCA, aes(PC1, PC2, color = as.factor(pop), shape = as.factor(status))) + geom_point(size = 3) + coord_equal() +\n",
    "   geom_hline(yintercept = 0, linetype=\"dotted\") + \n",
    "   geom_vline(xintercept = 0, linetype=\"dotted\") +\n",
    "   theme_light() + xlab(paste0(\"PC1 (\", signif(pve$pve[1], 3), \"%)\")) + ylab(paste0(\"PC2 (\", signif(pve$pve[2], 3), \"%)\"))+\n",
    "   labs(color= \"Group\", shape = \"Status\") +\n",
    "   ggtitle(\"PCA of BSF populations\")\n",
    " \n",
    "ggsave(\"PCA2.png\", plot = p3, width = 8, height = 6, dpi = 300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab42d2af",
   "metadata": {},
   "source": [
    "### ADMIXTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ac160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#more information: https://speciationgenomics.github.io/ADMIXTURE/\n",
    "FILE=\"output\"\n",
    "VCF=\"path/to/vcffile\"\n",
    "\n",
    "#Generate the input file in plink format\n",
    "plink-1.9-rc --vcf $VCF --make-bed --double-id --out $FILE --allow-extra-chr\n",
    "\n",
    "#ADMIXTURE does not accept chromosome names that are not human chromosomes. We will thus just exchange the first column by 0\n",
    "awk '{$1=\"0\";print $0}' $FILE.bim > $FILE.bim.tmp\n",
    "mv $FILE.bim.tmp $FILE.bim\n",
    "\n",
    "#Run admixture with cross-validation (k=2..k=5)\n",
    "for i in {2..11}\n",
    "do\n",
    "   admixture --cv $FILE.bed $i > log${i}.out\n",
    "done\n",
    "\n",
    "#To identify the best value of k clusters which is the value with lowest cross-validation error, we need to collect the cv errors.\n",
    "awk '/CV/ {print $3,$4}' *out | cut -c 4,7-20 > $FILE.cv.error\n",
    "\n",
    "awk '{split($1,name,\".\"); print $1,name[2]}' $FILE.nosex > $FILE.list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47641f08",
   "metadata": {},
   "source": [
    "## Phylogenetic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37febb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "VCF_File=\"path/to/vcf\"\n",
    "\n",
    "tassel-5.2.89-0/run_pipeline.pl -Xmx100G -SortGenotypeFilePlugin -inputFile \"$VCF_File\" -outputFile sorted.vcf -fileType VCF\n",
    "\n",
    "tassel-5.2.89-0/run_pipeline.pl -Xmx100G -importGuess sorted.vcf -ExportPlugin -saveAs sequences.phy -format Phylip_Inter\n",
    "\n",
    "iqtree -s sequences.phy -st DNA -m MFP -o PTA_000 -bb 1000 --bnni -cmax 15 -nt AUTO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f04ab9",
   "metadata": {},
   "source": [
    "## Additional Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36a8292-a43d-4c2e-b7ea-683cefbd19d4",
   "metadata": {},
   "source": [
    "### Treemix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f00db-46ca-4500-b7a3-2fc50a6da56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#more information: https://speciationgenomics.github.io/Treemix/\n",
    "\n",
    "for i in {0..5}\n",
    "do\n",
    " treemix -i BSF.noN.treemix.frq.gz -m $i -o BSF.$i -root OG -bootstrap -k 500 -noss  > treemix_${i}_log\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66285f6-b2ff-4c2b-8b17-ae727eb15940",
   "metadata": {},
   "source": [
    "### Genetic Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830e0cd3-e956-4e53-9d8c-eeb567b170bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#more information: https://github.com/simonhmartin/genomics_general\n",
    "VCF=\"path/to/vcffile\"\n",
    "\n",
    "#Step 1: Nucleotide diversity\n",
    "python3.7 genomics_general/parseVCF.py -i '$VCF' | bgzip --threads 8 > output.geno.gz\n",
    "\n",
    "python2.7 genomics_general/popgenWindows.py \\\n",
    "    -g output.geno.gz \\\n",
    "    -o output.Fst.Dxy.pi.csv.gz \\\n",
    "    -T 8 \\\n",
    "    -f phased \\\n",
    "    -w 100000 \\\n",
    "    -m 100 \\\n",
    "    -s 100000 \\\n",
    "    $(awk '{print \"-p\", $1}' pops.txt) \\\n",
    "    --popsFile sample_pop.txt\n",
    "\n",
    "#Step 2: FST and DXY\n",
    "python2.7 genomics_general/popgenWindows.py \\\n",
    "    -g output.geno.gz \\\n",
    "    -o output.Fst.Dxy.pi.csv.gz \\\n",
    "    -T 8 \\\n",
    "    -f phased \\\n",
    "    -w 50000 \\\n",
    "    -m 50 \\\n",
    "    -s 50000 \\\n",
    "    $(awk '{print \"-p\", $1}' pops.txt) \\\n",
    "    --popsFile sample_pop.txt\n",
    "\n",
    "#Step 3: Tajimas D\n",
    "# Generate population-specific sample lists\n",
    "while read pop; do\n",
    "    grep -w \"$pop\" sample_pop.txt | cut -f1 > ${pop}.txt\n",
    "done < pops.txt\n",
    "\n",
    "# Step 3: Calculate Tajima's D for the given population\n",
    "vcftools --gzvcf $VCF \\\n",
    "         --keep ${pop}.txt \\\n",
    "         --TajimaD 100000 \\\n",
    "         --out tajimasD_${pop}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d87c268-3407-4ec0-b48d-037651933f62",
   "metadata": {},
   "source": [
    "### Heterozygosity/Inbreeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb697419-a830-49de-b1ff-f366acb0561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#Step 1: Site frequency spectrum (SFS)\n",
    "RENAME_FILE=$1   # Sample mapping file\n",
    "ANGSD_OUT_DIR=$2 # Directory where ANGSD SAF outputs are stored\n",
    "SFS_OUT_DIR=$3   # Directory to store estimated SFS outputs\n",
    "REAL_SFS=\"angsd/misc/realSFS\"  # Path to realSFS binary\n",
    "\n",
    "# Ensure output directory exists\n",
    "mkdir -p \"$SFS_OUT_DIR\"\n",
    "\n",
    "# Check if arguments are provided\n",
    "if [[ -z \"$RENAME_FILE\" || -z \"$ANGSD_OUT_DIR\" || -z \"$SFS_OUT_DIR\" ]]; then\n",
    "    echo \"Usage: $0 <sample_bam_mapping.txt> <angsd_output_dir> <sfs_output_dir>\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Get the total number of samples in the mapping file\n",
    "TOTAL_SAMPLES=$(wc -l < \"$RENAME_FILE\")\n",
    "\n",
    "# Ensure SLURM task ID does not exceed available samples\n",
    "if [[ $SLURM_ARRAY_TASK_ID -gt $TOTAL_SAMPLES ]]; then\n",
    "    echo \"Error: Task ID exceeds the number of samples.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Extract sample name for this array job\n",
    "SAMPLE=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" \"$RENAME_FILE\")\n",
    "\n",
    "# Define input and output paths\n",
    "SAF_FILE=\"${ANGSD_OUT_DIR}/${SAMPLE}_angsd.saf.idx\"\n",
    "SFS_FILE=\"${SFS_OUT_DIR}/${SAMPLE}_est.ml\"\n",
    "\n",
    "# Check if SAF file exists\n",
    "if [[ ! -f \"$SAF_FILE\" ]]; then\n",
    "    echo \"Error: SAF file not found for sample $SAMPLE ($SAF_FILE)\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Run realSFS to Estimate SFS\n",
    "echo \"Estimating SFS for sample: $SAMPLE\"\n",
    "$REAL_SFS \"$SAF_FILE\" -P 1 > \"$SFS_FILE\"\n",
    "\n",
    "# Check if SFS estimation was successful\n",
    "if [[ ! -f \"$SFS_FILE\" ]]; then\n",
    "    echo \"Error: SFS file not created for sample $SAMPLE ($SFS_FILE)\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "#Step2: ANGSD heterozygosity\n",
    "RENAME_FILE=$1   # Sample mapping file (sample_name <TAB> bam_file_path)\n",
    "ANGSD_PATH=\"path/to/angsd\"\n",
    "REF_GENOME=\"path/to/ref\"\n",
    "ANGSD_OUT_DIR=$2 # Directory to store ANGSD outputs\n",
    "\n",
    "# Ensure output directory exists\n",
    "mkdir -p \"$ANGSD_OUT_DIR\"\n",
    "\n",
    "# Check if arguments are provided\n",
    "if [[ -z \"$RENAME_FILE\" || -z \"$ANGSD_OUT_DIR\" ]]; then\n",
    "    echo \"Usage: $0 <sample_bam_mapping.txt> <angsd_output_dir>\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Get the total number of samples in the mapping file\n",
    "TOTAL_SAMPLES=$(wc -l < \"$RENAME_FILE\")\n",
    "\n",
    "# Ensure SLURM task ID does not exceed available samples\n",
    "if [[ $SLURM_ARRAY_TASK_ID -gt $TOTAL_SAMPLES ]]; then\n",
    "    echo \"Error: Task ID exceeds the number of samples.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Extract sample name and BAM path for this array job\n",
    "SAMPLE_INFO=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" \"$RENAME_FILE\")\n",
    "SAMPLE=$(echo \"$SAMPLE_INFO\" | awk '{print $1}')\n",
    "BAM_PATH=$(echo \"$SAMPLE_INFO\" | awk '{print $2}')\n",
    "\n",
    "# Check if BAM file exists\n",
    "if [[ ! -f \"$BAM_PATH\" ]]; then\n",
    "    echo \"Error: BAM file for sample $SAMPLE not found ($BAM_PATH)\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Run ANGSD to Compute SAF\n",
    "echo \"Processing sample: $SAMPLE with BAM file: $BAM_PATH\"\n",
    "$ANGSD_PATH -i \"$BAM_PATH\" -anc \"$REF_GENOME\" -ref \"$REF_GENOME\" \\\n",
    "    -C 50 -minQ 20 -dosaf 1 -GL 1 -nThreads 4 -out \"${ANGSD_OUT_DIR}/${SAMPLE}_angsd\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ded80d3-d3ce-483a-82c9-a0f3c3ac19c6",
   "metadata": {},
   "source": [
    "### ROH_FROH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d20fa74-df15-420e-b62d-17a73f47c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash \n",
    "\n",
    "#Runs of homozygosity\n",
    "\n",
    "VCF='path/to/vcffile'\n",
    "\n",
    "# Convert VCF directly to BED format\n",
    "plink-1.9-rc --vcf $VCF --make-bed --keep-allele-order --nonfounders --allow-no-sex --double-id --out output\n",
    "\n",
    "#Run ROH Analysis in PLINK\n",
    "\n",
    "#--homozyg: Enables ROH detection.\n",
    "#--homozyg-snp 50: Minimum number of SNPs in an ROH.\n",
    "#--homozyg-kb 1000: Minimum ROH length (1 Mb).\n",
    "#--homozyg-density 50: Max density (1 SNP per 50 kb).\n",
    "#--homozyg-gap 100: Max gap between SNPs in an ROH.\n",
    "\n",
    "for pop in $(cat pops.txt); do\n",
    "    echo \"Processing ROH for population: $pop\"\n",
    "    plink-1.9-rc --bfile output --keep <(awk -v p=\"$pop\" '$2==p {print $1, $1}' sample_pop.txt) \\\n",
    "          --homozyg --homozyg-window-het 3 --homozyg-window-missing 20 \\\n",
    "          --out \"${pop}_roh\" &\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70bd1c0-88bf-4b0a-9621-f0ef24645a90",
   "metadata": {},
   "source": [
    "### Relatedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76ac6a0-b0f7-4c2c-b769-b7159b86d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash \n",
    "\n",
    "VCF='path/to/vcffile'\n",
    "\n",
    "#Relatedness\n",
    "plink-1.9-rc --vcf $VCF --make-bed --out output\n",
    "king -b output.bed --related"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76076a96-6c29-420a-868e-8fbc03220ea5",
   "metadata": {},
   "source": [
    "### LD and Effective Population Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd79615-e37e-48fc-8c4e-8ef2669b3f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash \n",
    "\n",
    "VCF='path/to/vcffile'\n",
    "\n",
    "#Linkage Disequilibrium (LD)\n",
    "PopLDdecay -InVCF $VCF -OutStat POP -MaxDist 500 -MAF 0.05\n",
    "\n",
    "#Effective population size (Ne)\n",
    "SNeP1.1 -ped output.ped -map output.map -phased -out output_Ne -threads 8 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdcdce2",
   "metadata": {},
   "source": [
    "## ðŸ“Š Expected Outputs\n",
    "- Filtered BAM and VCF files\n",
    "- PCA and ADMIXTURE plots\n",
    "- Phylogenetic tree files\n",
    "- Genetic diversity summary statistics\n",
    "- ROH and relatedness plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76430d19",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Tips\n",
    "- Adapt file paths and script names to your directory structure.\n",
    "- Check for errors and inspect log files after each step.\n",
    "- Keep track of software versions for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca46cbe-25f7-43d5-be56-cddff6a4791f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "ðŸ“« **Contact**\n",
    "\n",
    "For questions, suggestions, or contributions, please contact: \\\n",
    "[Peter Muchina] â€” [kimanimuchina@gmail.com]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
